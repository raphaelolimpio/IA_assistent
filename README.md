<!DOCTYPE html>
<html lang="pt-BR">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Aurora: Documenta√ß√£o Completa do Projeto</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #F8F9FA;
            color: #212529;
            scroll-behavior: smooth;
        }
        .chart-container {
            position: relative;
            width: 100%;
            max-width: 600px;
            margin-left: auto;
            margin-right: auto;
            height: 300px;
            max-height: 350px;
        }
        @media (min-width: 768px) {
            .chart-container {
                height: 350px;
                max-height: 400px;
            }
        }
        .section-card {
            background-color: #FFFFFF;
            border-radius: 0.5rem;
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.05), 0 2px 4px -1px rgba(0, 0, 0, 0.03);
            padding: 1.5rem;
            margin-bottom: 2rem;
        }
        .section-title {
            font-size: 2rem;
            font-weight: 700;
            color: #343A40;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 2px solid #6C757D;
        }
        .subsection-title {
            font-size: 1.5rem;
            font-weight: 600;
            color: #495057;
            margin-top: 1.5rem;
            margin-bottom: 0.75rem;
        }
        .content-text {
            font-size: 1rem;
            line-height: 1.6;
            color: #495057;
        }
        .hero-bg {
            background-color: #6C757D;
            color: #F8F9FA;
        }
        .hero-title {
            font-size: 2.5rem;
            font-weight: 700;
        }
        .hero-subtitle {
            font-size: 1.25rem;
            color: #E9ECEF;
        }
        .tree-view ul {
            list-style-type: none;
            padding-left: 1.5rem;
            position: relative;
        }
        .tree-view li {
            margin: 0.5rem 0;
            position: relative;
        }
        .tree-view li::before {
            content: '';
            position: absolute;
            top: -0.6em;
            left: -1rem;
            border-left: 1px solid #A0AEC0;
            height: 100%;
            width: 1px;
        }
        .tree-view li:last-child::before {
            height: 1.2em;
        }
        .tree-view li > span::before {
            content: '';
            position: absolute;
            top: 0.6em;
            left: -1rem;
            border-top: 1px solid #A0AEC0;
            width: 0.75rem;
            height: 0;
        }
        .tree-view .folder > span { font-weight: 600; color: #343A40; }
        .tree-view .file > span { color: #495057; }
        .tree-view .icon { margin-right: 0.5rem; color: #A0AEC0; }

        .accordion-button {
            background-color: #E9ECEF;
            color: #343A40;
            cursor: pointer;
            padding: 1rem;
            width: 100%;
            text-align: left;
            border: none;
            border-radius: 0.25rem;
            outline: none;
            transition: background-color 0.3s ease;
            font-weight: 600;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }
        .accordion-button:hover {
            background-color: #CED4DA;
        }
        .accordion-content {
            padding: 0 1rem;
            background-color: white;
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.3s ease-out, padding 0.3s ease-out;
            border: 1px solid #E9ECEF;
            border-top: none;
            border-radius: 0 0 0.25rem 0.25rem;
        }
        .accordion-content.open {
            padding: 1rem;
        }
        .arrow-icon {
            transition: transform 0.3s ease;
        }
        .arrow-icon.open {
            transform: rotate(90deg);
        }
        .flow-step {
            display: flex;
            align-items: flex-start;
            margin-bottom: 1rem;
            padding: 1rem;
            background-color: #E9ECEF;
            border-radius: 0.375rem;
        }
        .flow-step-number {
            font-size: 1.5rem;
            font-weight: 700;
            color: #6C757D;
            margin-right: 1rem;
            flex-shrink: 0;
            width: 2rem;
            text-align: center;
        }
        .flow-step-content {
            flex-grow: 1;
        }
        .flow-step-content strong { color: #343A40; }
        .code-block {
            background-color: #212529;
            color: #F8F9FA;
            padding: 1rem;
            border-radius: 0.25rem;
            overflow-x: auto;
            font-family: 'Courier New', Courier, monospace;
            font-size: 0.9rem;
            white-space: pre;
        }
        .chat-container {
            background-color: #F0F2F5;
            border-radius: 0.5rem;
            padding: 1rem;
            display: flex;
            flex-direction: column;
            height: 450px;
            overflow-y: auto;
            margin-bottom: 1rem;
        }
        .chat-message {
            margin-bottom: 0.75rem;
            padding: 0.5rem 0.75rem;
            border-radius: 0.5rem;
            max-width: 80%;
            word-wrap: break-word;
        }
        .chat-message.user {
            background-color: #E0F2F7;
            align-self: flex-end;
        }
        .chat-message.aurora {
            background-color: #D9EDF7;
            align-self: flex-start;
        }
        .chat-input-area {
            display: flex;
            gap: 0.5rem;
            margin-top: 1rem;
        }
        .chat-input-area textarea {
            flex-grow: 1;
            border: 1px solid #CED4DA;
            border-radius: 0.25rem;
            padding: 0.5rem;
            resize: vertical;
            min-height: 40px;
        }
        .chat-loading-indicator {
            text-align: center;
            margin-top: 0.5rem;
            font-style: italic;
            color: #6C757D;
        }
        .llm-button-group {
            display: flex;
            gap: 0.5rem;
            margin-top: 1rem;
            flex-wrap: wrap;
        }
        .llm-button-group button {
            flex-shrink: 0;
        }
    </style>
</head>
<body class="antialiased">

    <header class="hero-bg py-12 px-4 text-center">
        <div class="container mx-auto">
            <h1 class="hero-title mb-3">Aurora: Documenta√ß√£o Completa do Projeto</h1>
            <p class="hero-subtitle">Explore a arquitetura e o funcionamento do assistente virtual Aurora de forma clara e organizada.</p>
        </div>
    </header>

    <nav class="sticky top-0 bg-white shadow-md z-50">
        <div class="container mx-auto px-4">
            <div class="flex justify-center space-x-4 sm:space-x-8 py-3 overflow-x-auto whitespace-nowrap">
                <a href="#visao-geral" class="text-gray-600 hover:text-[#6C757D] font-medium">Vis√£o Geral</a>
                <a href="#estrutura" class="text-gray-600 hover:text-[#6C757D] font-medium">Estrutura</a>
                <a href="#modulos" class="text-gray-600 hover:text-[#6C757D] font-medium">M√≥dulos</a>
                <a href="#fluxo-operacional" class="text-gray-600 hover:text-[#6C757D] font-medium">Fluxo</a>
                <a href="#configuracao" class="text-gray-600 hover:text-[#6C757D] font-medium">Configura√ß√£o</a>
                <a href="#padroes-design" class="text-gray-600 hover:text-[#6C757D] font-medium">Padr√µes</a>
                <a href="#consideracoes-finais" class="text-gray-600 hover:text-[#6C757D] font-medium">Finais</a>
                <a href="#perguntar-aurora" class="text-gray-600 hover:text-[#6C757D] font-medium">Perguntar √† Aurora ‚ú®</a>
            </div>
        </div>
    </nav>

    <main class="container mx-auto p-4 md:p-8">

        <section id="visao-geral" class="section-card">
            <h2 class="section-title">Vis√£o Geral do Projeto</h2>
            <p class="content-text mb-4">
                O projeto <strong>Aurora</strong> √© um assistente virtual interativo desenvolvido em Python, com foco em comunica√ß√£o por voz (Speech-to-Text e Text-to-Speech) e na utiliza√ß√£o de intelig√™ncia artificial (Large Language Model - Google Gemini) para processar requisi√ß√µes complexas e interagir com ferramentas externas, como APIs de clima e pesquisa web.
            </p>
            <h3 class="subsection-title">Objetivo</h3>
            <p class="content-text mb-4">
                Criar um assistente prestativo, com uma personalidade entusiasta e criativa, capaz de responder a perguntas, fornecer informa√ß√µes contextuais e realizar buscas din√¢micas, gerenciando eficientemente o uso de recursos de API.
            </p>
            <h3 class="subsection-title">Tecnologias Principais</h3>
            <ul class="list-disc list-inside content-text space-y-1">
                <li><span class="icon">üêç</span>Python: Linguagem de programa√ß√£o principal.</li>
                <li><span class="icon">‚ú®</span>Google Gemini API: Para a intelig√™ncia do LLM e integra√ß√£o com ferramentas.</li>
                <li><span class="icon">üé§</span>Speech Recognition: Transcri√ß√£o de fala para texto.</li>
                <li><span class="icon">üó£Ô∏è</span>gTTS (Google Text-to-Speech): Convers√£o de texto para fala.</li>
                <li><span class="icon">üîä</span>Sounddevice & Soundfile: Para reprodu√ß√£o de √°udio de baixa lat√™ncia.</li>
                <li><span class="icon">üå¶Ô∏è</span>WeatherAPI.com: Para dados clim√°ticos.</li>
                <li><span class="icon">üîç</span>Google Custom Search API: Para pesquisa web.</li>
                <li><span class="icon">üåê</span>Requests: Para requisi√ß√µes HTTP a APIs externas.</li>
                <li><span class="icon">‚öôÔ∏è</span>python-dotenv: Para gerenciamento seguro de vari√°veis de ambiente.</li>
            </ul>
        </section>

        <section id="estrutura" class="section-card">
            <h2 class="section-title">Estrutura do Projeto</h2>
            <p class="content-text mb-4">
                A arquitetura do Aurora √© baseada em um design modular e desacoplado, facilitando a manuten√ß√£o, escalabilidade e a adi√ß√£o de novas funcionalidades. Abaixo, a estrutura de pastas do projeto e uma visualiza√ß√£o da distribui√ß√£o de componentes.
            </p>
            <div class="grid grid-cols-1 md:grid-cols-2 gap-8 items-start">
                <div>
                    <h3 class="subsection-title">Estrutura de Pastas</h3>
                    <div class="tree-view bg-gray-50 p-4 rounded-md text-sm">
                        <ul>
                            <li class="folder"><span class="icon">üìÅ</span>.
                                <ul>
                                    <li class="folder"><span class="icon">üìÅ</span>audio/
                                        <ul>
                                            <li class="file"><span class="icon">üìÑ</span>speak.py</li>
                                        </ul>
                                    </li>
                                    <li class="folder"><span class="icon">üìÅ</span>conscience/
                                        <ul>
                                            <li class="file"><span class="icon">üìÑ</span>__init__.py</li>
                                            <li class="file"><span class="icon">üìÑ</span>context_manager.py</li>
                                            <li class="file"><span class="icon">üìÑ</span>persona_manager.py</li>
                                            <li class="file"><span class="icon">üìÑ</span>usage_tracker.py</li>
                                        </ul>
                                    </li>
                                    <li class="folder"><span class="icon">üìÅ</span>core/
                                        <ul>
                                            <li class="file"><span class="icon">üìÑ</span>__init__.py</li>
                                            <li class="file"><span class="icon">üìÑ</span>assistant.py</li>
                                        </ul>
                                    </li>
                                    <li class="folder"><span class="icon">üìÅ</span>llm/
                                        <ul>
                                            <li class="file"><span class="icon">üìÑ</span>__init__.py</li>
                                            <li class="file"><span class="icon">üìÑ</span>generator.py</li>
                                        </ul>
                                    </li>
                                    <li class="folder"><span class="icon">üìÅ</span>memory/
                                        <ul>
                                            <li class="file"><span class="icon">üìÑ</span>__init__.py</li>
                                            <li class="file"><span class="icon">üìÑ</span>chat_history.py</li>
                                        </ul>
                                    </li>
                                    <li class="folder"><span class="icon">üìÅ</span>nlp/
                                        <ul>
                                            <li class="file"><span class="icon">üìÑ</span>__init__.py</li>
                                            <li class="file"><span class="icon">üìÑ</span>processing.py</li>
                                        </ul>
                                    </li>
                                    <li class="folder"><span class="icon">üìÅ</span>tools/
                                        <ul>
                                            <li class="file"><span class="icon">üìÑ</span>__init__.py</li>
                                            <li class="file"><span class="icon">üìÑ</span>weather_tool.py</li>
                                            <li class="file"><span class="icon">üìÑ</span>web_search_tool.py</li>
                                        </ul>
                                    </li>
                                    <li class="file"><span class="icon">üìÑ</span>main.py</li>
                                    <li class="file"><span class="icon">üîí</span>.env</li>
                                </ul>
                            </li>
                        </ul>
                    </div>
                </div>
                <div>
                    <h3 class="subsection-title">Contagem de Componentes por M√≥dulo</h3>
                    <div class="chart-container h-[300px] md:h-[380px]">
                        <canvas id="moduleComponentsChart"></canvas>
                    </div>
                    <p class="text-xs text-center mt-2 text-gray-500">Este gr√°fico de barras ilustra o n√∫mero de arquivos Python principais (excluindo `__init__.py`) em cada diret√≥rio de m√≥dulo principal, oferecendo uma vis√£o da distribui√ß√£o de funcionalidade.</p>
                </div>
            </div>
        </section>

        <section id="modulos" class="section-card">
            <h2 class="section-title">Detalhamento dos M√≥dulos e Racional de Escolha</h2>
            <p class="content-text mb-6">
                Cada m√≥dulo do projeto Aurora tem um prop√≥sito espec√≠fico. Clique em cada um para expandir e ver mais detalhes sobre suas fun√ß√µes e depend√™ncias, e o racional por tr√°s de sua escolha. Use o bot√£o "Resumir Se√ß√£o" para um resumo r√°pido via IA!
            </p>
            
            <div class="space-y-3">
                <div>
                    <button class="accordion-button">
                        <span><code>audio/speak.py</code> - Comunica√ß√£o Vocal</span>
                        <span class="arrow-icon">‚ñ∂</span>
                    </button>
                    <div class="accordion-content">
                        <p class="content-text mt-2"><strong>Prop√≥sito:</strong> Habilitar a comunica√ß√£o vocal bidirecional (usu√°rio -> assistente e assistente -> usu√°rio).</p>
                        <p class="content-text"><strong>Fun√ß√µes Principais:</strong> <code>ouvir_microfone()</code> (Speech-to-Text) e <code>falar()</code> (Text-to-Speech).</p>
                        <p class="content-text"><strong>Depend√™ncias:</strong> <code>speech_recognition</code>, <code>gTTS</code>, <code>soundfile</code>, <code>sounddevice</code>, <code>os</code>, <code>time</code>.</p>
                        <p class="content-text"><strong>Racional de Escolha:</strong></p>
                        <ul class="list-disc list-inside content-text pl-4">
                            <li><code>speech_recognition</code>: Biblioteca popular e flex√≠vel, com suporte a v√°rias APIs de reconhecimento de fala, incluindo Google Speech Recognition, que oferece boa precis√£o para portugu√™s. Inclui timeout e phrase_time_limit para controle do fluxo da escuta.</li>
                            <li><code>gTTS</code>: Simples e eficaz para gerar √°udio a partir de texto usando a voz do Google, com suporte a v√°rias l√≠nguas.</li>
                            <li><code>soundfile</code> e <code>sounddevice</code>: Escolhidos por sua robustez e capacidade de fornecer controle de baixo n√≠vel sobre a reprodu√ß√£o de √°udio. Eles resolvem problemas de compatibilidade e de libera√ß√£o de recursos de √°udio que ocorriam com outras bibliotecas (playsound, pydub), garantindo uma reprodu√ß√£o fluida e a continuidade do loop do assistente. Uma pequena pausa (<code>time.sleep</code>) √© adicionada ap√≥s a reprodu√ß√£o para garantir a libera√ß√£o do dispositivo de √°udio.</li>
                        </ul>
                        <p class="content-text mt-2"><strong>Observa√ß√µes:</strong> A cria√ß√£o de um novo objeto <code>sr.Recognizer()</code> a cada chamada de <code>ouvir_microfone()</code> ajuda a reiniciar os recursos do microfone, mitigando problemas de escuta cont√≠nua em alguns sistemas.</p>
                        <div class="llm-button-group">
                            <button class="summarize-button px-4 py-2 bg-[#A0AEC0] text-white rounded-md hover:bg-[#6C757D] transition-colors duration-200">Resumir Se√ß√£o ‚ú®</button>
                        </div>
                        <div class="summary-output mt-2 text-sm text-gray-700"></div>
                        <div class="loading-indicator hidden mt-2 text-sm text-[#6C757D]">Gerando...</div>
                    </div>
                </div>

                <div>
                    <button class="accordion-button">
                        <span><code>conscience/</code> - M√≥dulos de Consci√™ncia</span>
                        <span class="arrow-icon">‚ñ∂</span>
                    </button>
                    <div class="accordion-content">
                        <p class="content-text mt-2"><strong>Prop√≥sito:</strong> Prover ao assistente uma camada de "conhecimento" sobre si mesmo, o ambiente e suas capacidades, al√©m de gerenciar recursos de forma persistente.</p>
                        <ul class="list-disc list-inside content-text mt-2 space-y-1 pl-4">
                            <li><strong><code>context_manager.py</code>:</strong>
                                <p><strong>Prop√≥sito:</strong> Obter e gerenciar informa√ß√µes contextuais em tempo real, como data, hora e localiza√ß√£o geogr√°fica.</p>
                                <p><strong>Fun√ß√µes Principais:</strong> <code>get_current_time()</code>, <code>get_default_location()</code>.</p>
                                <p><strong>Depend√™ncias:</strong> <code>datetime</code>, <code>requests</code>, <code>json</code>.</p>
                                <p><strong>Racional:</strong> Informa√ß√µes contextuais (hora, localiza√ß√£o) tornam a intera√ß√£o mais natural e √∫til, permitindo respostas mais personalizadas sem que o usu√°rio precise fornecer esses dados. Uso de <code>ip-api.com</code> para obter localiza√ß√£o baseada em IP √© uma abordagem simples, gratuita e eficaz. Implementa um mecanismo de cache para otimizar o uso da API.</p>
                            </li>
                            <li><strong><code>persona_manager.py</code>:</strong>
                                <p><strong>Prop√≥sito:</strong> Definir e injetar a personalidade do assistente nas respostas do LLM.</p>
                                <p><strong>Fun√ß√µes Principais:</strong> <code>get_persona_prompt()</code>.</p>
                                <p><strong>Depend√™ncias:</strong> Nenhuma externa direta, apenas strings.</p>
                                <p><strong>Racional:</strong> Garante que o assistente "Aurora" mantenha uma voz e estilo consistentes em todas as intera√ß√µes delegadas ao LLM, tornando a experi√™ncia do usu√°rio mais agrad√°vel, engajadora e √∫nica. A inje√ß√£o da persona via prompt √© uma t√©cnica padr√£o de Prompt Engineering.</p>
                            </li>
                            <li><strong><code>usage_tracker.py</code>:</strong>
                                <p><strong>Prop√≥sito:</strong> Monitorar, limitar e persistir o uso de APIs externas para evitar custos excessivos ou viola√ß√µes de limites de requisi√ß√£o.</p>
                                <p><strong>Fun√ß√µes Principais:</strong> <code>increment_usage()</code>, <code>is_within_limit()</code>, <code>get_remaining_uses()</code>.</p>
                                <p><strong>Depend√™ncias:</strong> <code>datetime</code>, <code>json</code>, <code>os</code>.</p>
                                <p><strong>Racional:</strong> Essencial para a sustentabilidade e viabilidade do projeto, especialmente com APIs que imp√µem limites di√°rios de uso. O rastreamento em um arquivo JSON permite que o contador seja persistente entre as execu√ß√µes do programa.</p>
                            </li>
                        </ul>
                        <div class="llm-button-group">
                            <button class="summarize-button px-4 py-2 bg-[#A0AEC0] text-white rounded-md hover:bg-[#6C757D] transition-colors duration-200">Resumir Se√ß√£o ‚ú®</button>
                        </div>
                        <div class="summary-output mt-2 text-sm text-gray-700"></div>
                        <div class="loading-indicator hidden mt-2 text-sm text-[#6C757D]">Gerando...</div>
                    </div>
                </div>
                
                <div>
                    <button class="accordion-button">
                        <span><code>core/assistant.py</code> - L√≥gica Central (Orquestrador)</span>
                        <span class="arrow-icon">‚ñ∂</span>
                    </button>
                    <div class="accordion-content">
                        <p class="content-text mt-2"><strong>Prop√≥sito:</strong> Agrupar os componentes principais que definem o comportamento e o fluxo do assistente. Atua como o maestro, coordenando a intera√ß√£o entre todos os outros m√≥dulos.</p>
                        <p class="content-text"><strong>Fun√ß√µes Principais:</strong> <code>__init__()</code>, <code>_adicionar_ao_historico()</code>, <code>_obter_resposta()</code>, <code>run()</code>.</p>
                        <p class="content-text"><strong>Depend√™ncias:</strong> M√≥dulos internos (<code>audio</code>, <code>nlp</code>, <code>memory</code>, <code>conscience</code>, <code>llm</code>).</p>
                        <p class="content-text"><strong>Racional de Escolha:</strong> Implementa o loop principal de intera√ß√£o (<code>while True</code>). Delega tarefas especializadas para os m√≥dulos espec√≠ficos. Implementa um roteamento de inten√ß√µes h√≠brido:</p>
                        <ul class="list-disc list-inside content-text pl-4 mt-2">
                            <li><strong>Inten√ß√µes Locais:</strong> Para <code>saudacao</code>, <code>pedir_horas</code>, <code>agradecimento</code>, <code>despedida</code>, as respostas s√£o geradas localmente para maior velocidade e controle.</li>
                            <li><strong>Inten√ß√£o <code>pesquisar_web</code> (Controlada):</strong> Antes de envolver o LLM, o Assistant verifica o limite de uso di√°rio via <code>UsageTracker</code>. Se o limite for excedido, ele responde diretamente ao usu√°rio. Caso contr√°rio, delega a tarefa de pesquisa e resposta ao <code>LLMGenerator</code>.</li>
                            <li><strong>Outras Inten√ß√µes (Delegadas ao LLM):</strong> Para <code>pedir_clima</code> e <code>intencao_desconhecida</code> (qualquer outra pergunta), a requisi√ß√£o √© diretamente delegada ao <code>LLMGenerator</code>, que ent√£o utiliza o LLM e suas ferramentas para formular a resposta.</li>
                        </ul>
                        <p class="content-text mt-2">Promove o padr√£o de "controlador" que delega tarefas aos m√≥dulos especializados, mantendo o <code>assistant.py</code> focado no fluxo geral.</p>
                        <div class="llm-button-group">
                            <button class="summarize-button px-4 py-2 bg-[#A0AEC0] text-white rounded-md hover:bg-[#6C757D] transition-colors duration-200">Resumir Se√ß√£o ‚ú®</button>
                        </div>
                        <div class="summary-output mt-2 text-sm text-gray-700"></div>
                        <div class="loading-indicator hidden mt-2 text-sm text-[#6C757D]">Gerando...</div>
                    </div>
                </div>

                <div>
                    <button class="accordion-button">
                        <span><code>llm/generator.py</code> - M√≥dulo do Large Language Model</span>
                        <span class="arrow-icon">‚ñ∂</span>
                    </button>
                    <div class="accordion-content">
                        <p class="content-text mt-2"><strong>Prop√≥sito:</strong> Encapsular toda a l√≥gica de intera√ß√£o com o Google Gemini e gerenciar as ferramentas que o LLM pode utilizar.</p>
                        <p class="content-text"><strong>Fun√ß√µes Principais:</strong> <code>__init__()</code>, <code>generate_response()</code>.</p>
                        <p class="content-text"><strong>Depend√™ncias:</strong> <code>google.generativeai</code>, <code>os</code>, <code>dotenv</code>, <code>tools.weather_tool</code>, <code>tools.web_search_tool</code>, <code>conscience.persona_manager</code>, <code>conscience.usage_tracker</code>, <code>json</code>.</p>
                        <p class="content-text"><strong>Racional de Escolha:</strong></p>
                        <ul class="list-disc list-inside content-text pl-4">
                            <li><strong>Google Gemini (<code>models/gemini-1.5-flash-latest</code>):</strong> Escolhido por sua capacidade de processamento de linguagem natural, sua habilidade de seguir instru√ß√µes e, crucialmente, seu suporte nativo a <strong>chamadas de fun√ß√£o (Function Calling)</strong>. O modelo "flash" √© otimizado para velocidade.</li>
                            <li><strong>Function Calling Centralizado:</strong> O modelo Gemini √© configurado com a lista de <code>tools=[]</code> (passando as fun√ß√µes <code>get_weather_data</code> e <code>search_web</code>). Quando o LLM detecta que a inten√ß√£o do usu√°rio pode ser satisfeita por uma dessas ferramentas, ele gera uma <code>function_call</code>. O <code>LLMGenerator</code> intercepta essa chamada, executa a fun√ß√£o Python real e, ent√£o, injeta o resultado da ferramenta de volta no LLM para que ele formule a resposta final ao usu√°rio de forma natural.</li>
                            <li><strong>Controle de Uso de Busca na Web:</strong> O <code>UsageTracker</code> √© usado aqui para incrementar o contador de uso da API de busca apenas quando o Gemini realmente decide e executa a <code>search_web</code>.</li>
                            <li><strong><code>persona_manager</code>:</strong> Integrado para que o <code>LLMGenerator</code> sempre infunda a personalidade definida nos prompts enviados ao Gemini.</li>
                        </ul>
                        <div class="llm-button-group">
                            <button class="summarize-button px-4 py-2 bg-[#A0AEC0] text-white rounded-md hover:bg-[#6C757D] transition-colors duration-200">Resumir Se√ß√£o ‚ú®</button>
                        </div>
                        <div class="summary-output mt-2 text-sm text-gray-700"></div>
                        <div class="loading-indicator hidden mt-2 text-sm text-[#6C757D]">Gerando...</div>
                    </div>
                </div>

                <div>
                    <button class="accordion-button">
                        <span><code>memory/chat_history.py</code> - M√≥dulo de Mem√≥ria</span>
                        <span class="arrow-icon">‚ñ∂</span>
                    </button>
                    <div class="accordion-content">
                        <p class="content-text mt-2"><strong>Prop√≥sito:</strong> Manter um registro das intera√ß√µes passadas para dar ao assistente um senso de contexto e coer√™ncia.</p>
                        <p class="content-text"><strong>Fun√ß√µes Principais:</strong> <code>add_message()</code>, <code>get_history()</code>, <code>clear_history()</code>.</p>
                        <p class="content-text"><strong>Depend√™ncias:</strong> Nenhuma.</p>
                        <p class="content-text"><strong>Racional de Escolha:</strong> A capacidade de um LLM de manter uma conversa coerente e relevante depende criticamente de um hist√≥rico de conversa. O limite de 20 mensagens (<code>self._history = self._history[-20:]</code>) √© uma estrat√©gia comum para gerenciar o tamanho do contexto do LLM, equilibrando coer√™ncia da conversa, custos de tokens e lat√™ncia.</p>
                        <div class="llm-button-group">
                            <button class="summarize-button px-4 py-2 bg-[#A0AEC0] text-white rounded-md hover:bg-[#6C757D] transition-colors duration-200">Resumir Se√ß√£o ‚ú®</button>
                        </div>
                        <div class="summary-output mt-2 text-sm text-gray-700"></div>
                        <div class="loading-indicator hidden mt-2 text-sm text-[#6C757D]">Gerando...</div>
                    </div>
                </div>

                <div>
                    <button class="accordion-button">
                        <span><code>nlp/processing.py</code> - Processamento de Linguagem Natural</span>
                        <span class="arrow-icon">‚ñ∂</span>
                    </button>
                    <div class="accordion-content">
                        <p class="content-text mt-2"><strong>Prop√≥sito:</strong> Realizar a an√°lise inicial do texto de entrada do usu√°rio para identificar inten√ß√µes b√°sicas de forma eficiente.</p>
                        <p class="content-text"><strong>Fun√ß√µes Principais:</strong> <code>processar_texto()</code>.</p>
                        <p class="content-text"><strong>Depend√™ncias:</strong> Nenhuma.</p>
                        <p class="content-text"><strong>Racional de Escolha:</strong> Implementa uma estrat√©gia h√≠brida de NLP: utiliza regras simples (<code>if/elif</code> com palavras-chave) para inten√ß√µes de alta confian√ßa e que podem ser tratadas rapidamente (sauda√ß√£o, hora, agradecimentos, despedidas, e o gatilho para "pesquisa na web"). Para inten√ß√µes mais complexas ou amb√≠guas, a requisi√ß√£o √© delegada ao <code>LLMGenerator</code> para an√°lise sem√¢ntica mais profunda e execu√ß√£o de tarefas via LLM.</p>
                        <div class="llm-button-group">
                            <button class="summarize-button px-4 py-2 bg-[#A0AEC0] text-white rounded-md hover:bg-[#6C757D] transition-colors duration-200">Resumir Se√ß√£o ‚ú®</button>
                        </div>
                        <div class="summary-output mt-2 text-sm text-gray-700"></div>
                        <div class="loading-indicator hidden mt-2 text-sm text-[#6C757D]">Gerando...</div>
                    </div>
                </div>

                <div>
                    <button class="accordion-button">
                        <span><code>tools/</code> - M√≥dulos de Ferramentas</span>
                        <span class="arrow-icon">‚ñ∂</span>
                    </button>
                    <div class="accordion-content">
                        <p class="content-text mt-2"><strong>Prop√≥sito:</strong> Encapsular a l√≥gica para interagir com APIs e servi√ßos externos, estendendo as capacidades do assistente para al√©m do que o LLM pode fazer intrinsecamente.</p>
                        <ul class="list-disc list-inside content-text mt-2 space-y-1 pl-4">
                            <li><strong><code>weather_tool.py</code>:</strong>
                                <p><strong>Prop√≥sito:</strong> Interagir com a WeatherAPI.com para obter dados clim√°ticos.</p>
                                <p><strong>Fun√ß√µes Principais:</strong> <code>get_weather_data()</code>.</p>
                                <p><strong>Depend√™ncias:</strong> <code>requests</code>, <code>os</code>, <code>dotenv</code>, <code>json</code>.</p>
                                <p><strong>Racional:</strong> <code>requests</code> √© a biblioteca padr√£o para fazer requisi√ß√µes HTTP em Python, robusta e amplamente utilizada. A encapsula√ß√£o da l√≥gica de API em uma fun√ß√£o dedicada permite que o LLM a "chame" de forma program√°tica via Function Calling. Inclui tratamento de erros robusto para falhas na API.</p>
                            </li>
                            <li><strong><code>web_search_tool.py</code>:</strong>
                                <p><strong>Prop√≥sito:</strong> Interagir com a Google Custom Search API para realizar pesquisas web.</p>
                                <p><strong>Fun√ß√µes Principais:</strong> <code>search_web()</code>.</p>
                                <p><strong>Depend√™ncias:</strong> <code>requests</code>, <code>os</code>, <code>dotenv</code>.</p>
                                <p><strong>Racional:</strong> Permite ao assistente buscar informa√ß√µes na internet, estendendo significativamente seu "conhecimento" para al√©m dos dados de treinamento do LLM (que podem estar desatualizados ou n√£o cobrir tudo). Essencial para perguntas sobre eventos atuais, not√≠cias ou informa√ß√µes muito espec√≠ficas. Tamb√©m encapsulada para ser chamada pelo LLM.</p>
                            </li>
                        </ul>
                        <div class="llm-button-group">
                            <button class="summarize-button px-4 py-2 bg-[#A0AEC0] text-white rounded-md hover:bg-[#6C757D] transition-colors duration-200">Resumir Se√ß√£o ‚ú®</button>
                        </div>
                        <div class="summary-output mt-2 text-sm text-gray-700"></div>
                        <div class="loading-indicator hidden mt-2 text-sm text-[#6C757D]">Gerando...</div>
                    </div>
                </div>

                <div>
                    <button class="accordion-button">
                        <span><code>main.py</code> - Ponto de Entrada</span>
                        <span class="arrow-icon">‚ñ∂</span>
                    </button>
                    <div class="accordion-content">
                        <p class="content-text mt-2"><strong>Prop√≥sito:</strong> O ponto de entrada principal do aplicativo.</p>
                        <p class="content-text"><strong>Depend√™ncias:</strong> <code>core.assistant</code>.</p>
                        <p class="content-text"><strong>Racional:</strong> Segue a conven√ß√£o Python para iniciar a execu√ß√£o de um programa, mantendo-o simples e focado apenas na inicializa√ß√£o do assistente.</p>
                        <div class="llm-button-group">
                            <button class="summarize-button px-4 py-2 bg-[#A0AEC0] text-white rounded-md hover:bg-[#6C757D] transition-colors duration-200">Resumir Se√ß√£o ‚ú®</button>
                        </div>
                        <div class="summary-output mt-2 text-sm text-gray-700"></div>
                        <div class="loading-indicator hidden mt-2 text-sm text-[#6C757D]">Gerando...</div>
                    </div>
                </div>
            </div>
        </section>

        <section id="fluxo-operacional" class="section-card">
            <h2 class="section-title">Fluxo de Execu√ß√£o Detalhado: "Aurora, pesquise para mim o que √© computa√ß√£o qu√¢ntica."</h2>
            <p class="content-text mb-6">
                Para ilustrar como o Aurora funciona de ponta a ponta, vamos seguir o fluxo de uma pergunta que envolve o uso de uma ferramenta externa, como a pesquisa na web.
            </p>
            <div class="space-y-4">
                <div class="flow-step">
                    <div class="flow-step-number">1.</div>
                    <div class="flow-step-content">
                        <strong><code>main.py</code> (Ponto de Entrada):</strong> O script <code>main.py</code> √© executado, inicializando a classe <code>Assistant</code>.
                    </div>
                </div>
                <div class="flow-step">
                    <div class="flow-step-number">2.</div>
                    <div class="flow-step-content">
                        <strong><code>core/assistant.py</code> (Inicializa√ß√£o e Contexto):</strong>
                        <ul class="list-disc list-inside text-sm mt-1 pl-4">
                            <li>Instancia <code>ChatHistory</code>, <code>ContextManager</code> (que tenta obter a localiza√ß√£o em tempo real), <code>PersonaManager</code> e <code>UsageTracker</code> (para controle de busca).</li>
                            <li>Instancia <code>LLMGenerator</code>, passando a ele a inst√¢ncia de <code>ChatHistory</code>. O <code>LLMGenerator</code> inicializa o Gemini e registra as ferramentas <code>get_weather_data</code> e <code>search_web</code>.</li>
                        </ul>
                    </div>
                </div>
                <div class="flow-step">
                    <div class="flow-step-number">3.</div>
                    <div class="flow-step-content">
                        <strong><code>core/assistant.py</code> (<code>run()</code> - Loop Principal):</strong> O m√©todo <code>run()</code> entra em seu loop cont√≠nuo de intera√ß√£o. Uma mensagem como "Estou ouvindo... Diga algo:" √© exibida.
                    </div>
                </div>
                <div class="flow-step">
                    <div class="flow-step-number">4.</div>
                    <div class="flow-step-content">
                        <strong><code>audio/speak.py</code> (Ouvir Microfone):</strong>
                        <ul class="list-disc list-inside text-sm mt-1 pl-4">
                            <li><code>ouvir_microfone()</code> ativa o microfone e captura a fala do usu√°rio: "Aurora, pesquise para mim o que √© computa√ß√£o qu√¢ntica."</li>
                            <li>O √°udio √© enviado para o servi√ßo Google Speech Recognition e transcrito.</li>
                            <li>Retorna a frase em texto: "aurora, pesquise para mim o que √© computa√ß√£o qu√¢ntica."</li>
                        </ul>
                    </div>
                </div>
                <div class="flow-step">
                    <div class="flow-step-number">5.</div>
                    <div class="flow-step-content">
                        <strong><code>memory/chat_history.py</code> (Atualizar Hist√≥rico):</strong> A frase do usu√°rio √© adicionada ao hist√≥rico da conversa.
                    </div>
                </div>
                <div class="flow-step">
                    <div class="flow-step-number">6.</div>
                    <div class="flow-step-content">
                        <strong><code>nlp/processing.py</code> (Processar Inten√ß√£o):</strong>
                        <ul class="list-disc list-inside text-sm mt-1 pl-4">
                            <li>A frase √© passada para <code>processar_texto()</code>.</li>
                            <li>A fun√ß√£o detecta "pesquise para mim" e retorna a inten√ß√£o "pesquisar_web".</li>
                        </ul>
                    </div>
                </div>
                <div class="flow-step">
                    <div class="flow-step-number">7.</div>
                    <div class="flow-step-content">
                        <strong><code>core/assistant.py</code> (Tomada de Decis√£o e Controle de Uso):</strong>
                        <ul class="list-disc list-inside text-sm mt-1 pl-4">
                            <li>A inten√ß√£o "pesquisar_web" √© detectada.</li>
                            <li>O Assistant verifica <code>self.search_tracker.is_within_limit()</code>.</li>
                            <li><strong>Cen√°rio A: Limite Excedido:</strong> Se o contador de busca j√° atingiu o limite, o Assistant formula uma resposta direta informando o limite, adiciona-a ao hist√≥rico, e a IA fala essa mensagem, sem chamar o <code>LLMGenerator</code> ou a API de busca. O loop continua.</li>
                            <li><strong>Cen√°rio B: Dentro do Limite (Nosso Exemplo):</strong> Se o limite n√£o foi excedido, a requisi√ß√£o √© delegada ao <code>LLMGenerator</code>.</li>
                        </ul>
                    </div>
                </div>
                <div class="flow-step">
                    <div class="flow-step-number">8.</div>
                    <div class="flow-step-content">
                        <strong><code>llm/generator.py</code> (Gerar Resposta com LLM e Ferramenta):</strong>
                        <ul class="list-disc list-inside text-sm mt-1 pl-4">
                            <li><code>generate_response("aurora, pesquise para mim o que √© computa√ß√£o qu√¢ntica.")</code> √© chamada.</li>
                            <li>O prompt da persona e o hist√≥rico do chat s√£o usados para iniciar uma sess√£o com o Gemini.</li>
                            <li>O Gemini analisa a frase do usu√°rio e, pr√©-configurado com as ferramentas, reconhece que a pergunta pode ser respondida pela ferramenta <code>search_web</code>.</li>
                            <li>O LLM retorna uma <code>function_call</code> para <code>search_web</code> com <code>query="computa√ß√£o qu√¢ntica"</code>.</li>
                            <li>O <code>LLMGenerator</code> intercepta essa <code>function_call</code>.</li>
                            <li>Ele chama <code>self.search_tracker.increment_usage()</code> (incrementando o contador de uso da busca).</li>
                            <li>Ele executa a fun√ß√£o Python <code>search_web(query="computa√ß√£o qu√¢ntica")</code>.</li>
                        </ul>
                    </div>
                </div>
                <div class="flow-step">
                    <div class="flow-step-number">9.</div>
                    <div class="flow-step-content">
                        <strong><code>tools/web_search_tool.py</code> (Execu√ß√£o da Ferramenta):</strong>
                        <ul class="list-disc list-inside text-sm mt-1 pl-4">
                            <li><code>search_web("computa√ß√£o qu√¢ntica")</code> faz uma requisi√ß√£o HTTP para a Google Custom Search API.</li>
                            <li>Recebe os resultados da pesquisa (t√≠tulos, links, snippets).</li>
                            <li>Retorna uma string formatada com esses resultados.</li>
                        </ul>
                    </div>
                </div>
                <div class="flow-step">
                    <div class="flow-step-number">10.</div>
                    <div class="flow-step-content">
                        <strong><code>llm/generator.py</code> (Resposta Final do LLM):</strong>
                        <ul class="list-disc list-inside text-sm mt-1 pl-4">
                            <li>O <code>LLMGenerator</code> envia o resultado da ferramenta (a string com os resultados da pesquisa) de volta para a sess√£o do Gemini.</li>
                            <li>O Gemini, usando sua personalidade (Aurora), formula uma resposta amig√°vel e entusiasmada baseada nos resultados da pesquisa. Ex: "Que pergunta fant√°stica! A computa√ß√£o qu√¢ntica √© um campo revolucion√°rio que usa os princ√≠pios da mec√¢nica qu√¢ntica para resolver problemas complexos. √â um universo de possibilidades, com computadores que operam de formas que nem imaginamos. Pronta para mais descobertas?"</li>
                            <li>Esta resposta final √© retornada ao <code>assistant.py</code>.</li>
                        </ul>
                    </div>
                </div>
                <div class="flow-step">
                    <div class="flow-step-number">11.</div>
                    <div class="flow-step-content">
                        <strong><code>memory/chat_history.py</code> (Atualizar Hist√≥rico):</strong> A resposta final da IA √© adicionada ao hist√≥rico da conversa.
                    </div>
                </div>
                <div class="flow-step">
                    <div class="flow-step-number">12.</div>
                    <div class="flow-step-content">
                        <strong><code>audio/speak.py</code> (Falar Resposta):</strong> A resposta final do Gemini √© passada para <code>falar()</code>, que a converte em √°udio e a reproduz.
                    </div>
                </div>
                <div class="flow-step">
                    <div class="flow-step-number">13.</div>
                    <div class="flow-step-content">
                        <strong><code>core/assistant.py</code> (Continuar Loop):</strong> O assistente aguarda a pr√≥xima intera√ß√£o do usu√°rio, exibindo "Estou ouvindo... Diga algo:".
                    </div>
                </div>
            </div>
        </section>

        <section id="configuracao" class="section-card">
            <h2 class="section-title">Configura√ß√£o Essencial (`.env`)</h2>
            <p class="content-text mb-4">
                Para o correto funcionamento do Aurora, √© crucial configurar as chaves de API em um arquivo <code>.env</code> na raiz do projeto. Este arquivo <strong>n√£o deve</strong> ser enviado para reposit√≥rios de c√≥digo (ex: Git), para evitar a exposi√ß√£o de credenciais sens√≠veis.
            </p>
            <div class="code-block">
# Google Gemini API Key
GEMINI_API_KEY="YOUR_GOOGLE_GEMINI_API_KEY_HERE"

# WeatherAPI.com Key
WEATHERAPI_API_KEY="YOUR_WEATHERAPI_KEY_HERE"

# Google Custom Search API Key and Custom Search Engine ID
Google Search_API_KEY="YOUR_Google Search_API_KEY_HERE"
Google Search_CX="YOUR_Google Search_CX_ID_HERE"
            </div>
            <p class="content-text mt-4">
                <strong>Como obter estas chaves:</strong>
            </p>
            <ul class="list-disc list-inside content-text space-y-1 pl-4">
                <li><strong><code>GEMINI_API_KEY</code>:</strong> Obt√≠vel no <a href="https://aistudio.google.com/app/apikey" target="_blank" class="text-[#6C757D] hover:underline">Google AI Studio</a> ou Google Cloud Console (habilitar Gemini API).</li>
                <li><strong><code>WEATHERAPI_API_KEY</code>:</strong> Registre-se em <a href="https://www.weatherapi.com/" target="_blank" class="text-[#6C757D] hover:underline">www.weatherapi.com</a> para obter sua chave de API gratuita.</li>
                <li><strong><code>Google Search_API_KEY</code> e <code>Google Search_CX</code>:</strong>
                    <ul class="list-circle list-inside pl-4">
                        <li><strong>API Key:</strong> No <a href="https://console.cloud.google.com/" target="_blank" class="text-[#6C757D] hover:underline">Google Cloud Console</a>, crie um projeto, navegue at√© "APIs & Services" -> "Credenciais", e crie uma chave de API. Recomenda-se restringi-la √† "Custom Search API".</li>
                        <li><strong>CX ID:</strong> No <a href="https://programmablesearchengine.google.com/" target="_blank" class="text-[#6C757D] hover:underline">Google Programmable Search Engine</a>, crie um novo mecanismo de pesquisa (pode ser para <code>www.google.com</code> para toda a web), e voc√™ obter√° um "Search engine ID" (CX).</li>
                    </ul>
                </li>
            </ul>
        </section>

        <section id="padroes-design" class="section-card">
            <h2 class="section-title">Padr√µes de Design e Metodologias</h2>
            <p class="content-text mb-4">
                O projeto Aurora emprega diversos padr√µes e metodologias para garantir uma arquitetura robusta, modular e de f√°cil manuten√ß√£o. Esta se√ß√£o detalha as abordagens de design adotadas.
            </p>
            <h3 class="subsection-title">Arquitetura Modular / Camadas</h3>
            <p class="content-text mb-2">
                O projeto √© estruturado em m√≥dulos l√≥gicos (<code>audio</code>, <code>conscience</code>, <code>core</code>, <code>llm</code>, <code>memory</code>, <code>nlp</code>, <code>tools</code>), cada um com responsabilidades bem definidas. Isso promove:
            </p>
            <ul class="list-disc list-inside content-text space-y-1 pl-4">
                <li><strong>Alta Coes√£o:</strong> Fun√ß√µes e classes relacionadas s√£o agrupadas dentro de seus respectivos m√≥dulos.</li>
                <li><strong>Baixo Acoplamento:</strong> M√≥dulos s√£o independentes e se comunicam atrav√©s de interfaces bem definidas (chamadas de fun√ß√£o/m√©todo), minimizando depend√™ncias e facilitando a substitui√ß√£o ou atualiza√ß√£o de componentes sem afetar outras partes do sistema.</li>
            </ul>

            <h3 class="subsection-title">Inje√ß√£o de Depend√™ncia (via Construtor)</h3>
            <p class="content-text mb-2">
                Objetos de gerenciamento de estado e contexto (<code>ChatHistory</code>, <code>ContextManager</code>, <code>PersonaManager</code>, <code>UsageTracker</code>) e o <code>LLMGenerator</code> s√£o instanciados e passados para outros m√≥dulos (principalmente para o <code>Assistant</code> e <code>LLMGenerator</code>) via construtor. Isso:
            </p>
            <ul class="list-disc list-inside content-text space-y-1 pl-4">
                <li>Promove o acoplamento fraco, pois os m√≥dulos n√£o precisam criar suas pr√≥prias depend√™ncias, mas as recebem.</li>
                <li>Facilita testes unit√°rios, permitindo a inje√ß√£o de *mocks* (objetos simulados) em ambientes de teste.</li>
            </ul>

            <h3 class="subsection-title">Estrat√©gia H√≠brida de Processamento de Inten√ß√µes (Regras + LLM com Ferramentas)</h3>
            <p class="content-text mb-2">
                O sistema emprega uma abordagem em camadas para processar as inten√ß√µes do usu√°rio:
            </p>
            <ul class="list-disc list-inside content-text space-y-1 pl-4">
                <li><strong><code>nlp/processing.py</code> (Camada de Regras):</strong> Utiliza uma estrat√©gia baseada em regras (<code>if/elif</code> com palavras-chave) para detectar inten√ß√µes simples e de alta confian√ßa (sauda√ß√µes, horas, agradecimentos, despedidas) e a inten√ß√£o de "pesquisar_web". Isso otimiza o desempenho e economiza chamadas ao LLM para tarefas triviais.</li>
                <li><strong><code>core/assistant.py</code> (Camada de Roteamento/Controle):</strong> Atua como um router para as inten√ß√µes detectadas:
                    <ul class="list-circle list-inside pl-4">
                        <li><strong>Respostas Locais:</strong> Para inten√ß√µes como <code>saudacao</code> ou <code>pedir_horas</code>, o Assistant gera e fornece a resposta diretamente, sem envolver o LLM.</li>
                        <li><strong>Controle de Ferramentas com Limite (<code>pesquisar_web</code>):</strong> Antes de delegar uma pesquisa na web, o Assistant verifica o limite de uso di√°rio da API de busca via <code>UsageTracker</code>. Se o limite for excedido, ele responde diretamente ao usu√°rio. Caso contr√°rio, a requisi√ß√£o √© passada para o <code>LLMGenerator</code>.</li>
                        <li><strong>Delega√ß√£o ao LLM (<code>pedir_clima</code> e <code>intencao_desconhecida</code>):</strong> Para inten√ß√µes que exigem compreens√£o complexa ou uso de ferramentas pelo LLM (como <code>pedir_clima</code> ou qualquer pergunta geral), a requisi√ß√£o √© delegada ao <code>LLMGenerator</code>.</li>
                    </ul>
                </li>
                <li><strong><code>llm/generator.py</code> (Camada LLM e Ferramentas):</strong> Para inten√ß√µes delegadas, o <code>LLMGenerator</code> utiliza o LLM (Gemini) para an√°lise sem√¢ntica mais profunda, gera√ß√£o de respostas criativas e, crucialmente, para orquestrar o uso de ferramentas externas.</li>
            </ul>

            <h3 class="subsection-title">Function Calling (Google Gemini)</h3>
            <p class="content-text mb-2">
                Um dos padr√µes mais cr√≠ticos para a extensibilidade do assistente. O LLM (<code>models/gemini-1.5-flash-latest</code>) √© configurado com a descri√ß√£o das ferramentas externas dispon√≠veis (<code>get_weather_data</code>, <code>search_web</code>). Quando o LLM (<code>generate_response()</code> no <code>llm/generator.py</code>) detecta que a inten√ß√£o do usu√°rio pode ser satisfeita por uma dessas ferramentas, ele gera uma <code>function_call</code> (uma chamada de fun√ß√£o simulada com seus argumentos). O <code>LLMGenerator</code> intercepta essa chamada, executa a fun√ß√£o Python correspondente (da pasta <code>tools/</code>) e, ent√£o, injeta o resultado da ferramenta de volta no LLM para que ele formule a resposta final ao usu√°rio de forma natural, mantendo a personalidade. Isso permite ao LLM "agir" no mundo real e expandir seu conhecimento.
            </p>

            <h3 class="subsection-title">Gerenciamento de Estado de Conversa (Chat History)</h3>
            <p class="content-text mb-2">
                O <code>memory/chat_history.py</code> implementa um gerenciador de estado para a conversa√ß√£o. A manuten√ß√£o de um hist√≥rico (<code>_history</code>) √© vital para que o LLM compreenda o contexto e mantenha a coer√™ncia do di√°logo, permitindo que a IA se lembre de refer√™ncias anteriores. A estrat√©gia de "janela deslizante" (manter apenas as √∫ltimas 20 mensagens) √© um padr√£o comum para gerenciar o tamanho do contexto do LLM, equilibrando coer√™ncia, custos de tokens e lat√™ncia.
            </p>

            <h3 class="subsection-title">Gerenciamento de Recursos Persistente (Usage Tracker)</h3>
            <p class="content-text mb-2">
                O <code>conscience/usage_tracker.py</code> implementa um padr√£o de contador e limite di√°rio para APIs externas, persistindo os dados em um arquivo JSON (<code>Google Search_usage.json</code>). Isso √© fundamental para gerenciar o consumo de APIs que imp√µem limites de requisi√ß√£o ou t√™m custos associados, garantindo a operabilidade cont√≠nua do assistente sem interrup√ß√µes por exaust√£o de cota, e fornecendo feedback ao usu√°rio sobre seu uso.
            </p>

            <h3 class="subsection-title">Configura√ß√£o Externa (dotenv)</h3>
            <p class="content-text mb-2">
                O uso de <code>.env</code> para vari√°veis de ambiente (chaves de API) segue o princ√≠pio das "12 Fatores" (Twelve-Factor App), promovendo:
            </p>
            <ul class="list-disc list-inside content-text space-y-1 pl-4">
                <li><strong>Seguran√ßa:</strong> Chaves sens√≠veis n√£o s√£o commitadas para o controle de vers√£o (ex: GitHub).</li>
                <li><strong>Portabilidade:</strong> O c√≥digo pode ser implantado em diferentes ambientes (desenvolvimento, produ√ß√£o) sem modifica√ß√µes, apenas alterando o arquivo <code>.env</code>.</li>
            </ul>
        </section>

        <section id="consideracoes-finais" class="section-card">
            <h2 class="section-title">Considera√ß√µes Finais</h2>
            <p class="content-text mb-4">
                A arquitetura proposta para o assistente Aurora demonstra uma abordagem robusta para o desenvolvimento de sistemas de conversa√ß√£o inteligentes e interativos. A modularidade, a integra√ß√£o inteligente de LLMs com *function calling*, e a aten√ß√£o ao gerenciamento de recursos e estado (incluindo persist√™ncia para uso de API) s√£o cruciais para a constru√ß√£o de assistentes virtuais eficientes, escal√°veis e amig√°veis ao usu√°rio.
            </p>
            <p class="content-text">
                Futuras melhorias podem incluir a adi√ß√£o de mais ferramentas, um sistema de hotword para ativa√ß√£o do assistente, e uma interface de usu√°rio mais sofisticada.
            </p>
        </section>

        <section id="perguntar-aurora" class="section-card">
            <h2 class="section-title">Perguntar √† Aurora ‚ú®</h2>
            <p class="content-text mb-4">
                Tem alguma d√∫vida espec√≠fica sobre o projeto Aurora? Pergunte √† nossa assistente de IA, e ela tentar√° responder com base nesta documenta√ß√£o!
            </p>
            <div id="chat-display" class="chat-container">
                <div class="chat-message aurora">Ol√°! Sou a Aurora, sua assistente de documenta√ß√£o. Como posso ajudar a explorar o projeto?</div>
            </div>
            <div class="chat-input-area">
                <textarea id="user-question" placeholder="Digite sua pergunta aqui..." rows="2"></textarea>
                <button id="ask-aurora-button" class="px-4 py-2 bg-[#6C757D] text-white rounded-md hover:bg-[#495057] transition-colors duration-200">Perguntar ‚ú®</button>
            </div>
            <div id="chat-loading-indicator" class="chat-loading-indicator hidden">Aurora est√° pensando...</div>
            <div id="full-doc-text" class="hidden">
                <p>O projeto Aurora √© um assistente virtual interativo desenvolvido em Python, com foco em comunica√ß√£o por voz (Speech-to-Text e Text-to-Speech) e na utiliza√ß√£o de intelig√™ncia artificial (Large Language Model - Google Gemini) para processar requisi√ß√µes complexas e interagir com ferramentas externas, como APIs de clima e pesquisa web.</p>
                <p>Objetivo: Criar um assistente prestativo, com uma personalidade entusiasta e criativa, capaz de responder a perguntas, fornecer informa√ß√µes contextuais e realizar buscas din√¢micas, gerenciando eficientemente o uso de recursos de API.</p>
                <p>Tecnologias Principais: Python, Google Gemini API, Speech Recognition, gTTS (Google Text-to-Speech), Sounddevice & Soundfile, WeatherAPI.com, Google Custom Search API, Requests, python-dotenv.</p>
                <p>Estrutura do Projeto: O projeto √© organizado em m√≥dulos l√≥gicos: audio/ (speak.py), conscience/ (context_manager.py, persona_manager.py, usage_tracker.py), core/ (assistant.py), llm/ (generator.py), memory/ (chat_history.py), nlp/ (processing.py), tools/ (weather_tool.py, web_search_tool.py), main.py e .env.</p>
                <p>Detalhes dos M√≥dulos:</p>
                <p>audio/speak.py: Prop√≥sito: Comunica√ß√£o vocal bidirecional (STT e TTS). Fun√ß√µes: ouvir_microfone(), falar(). Depend√™ncias: speech_recognition, gTTS, soundfile, sounddevice, os, time. Racional: Robustez na reprodu√ß√£o de √°udio e rein√≠cio de recursos do microfone.</p>
                <p>conscience/: Prop√≥sito: Conhecimento sobre ambiente e recursos. context_manager.py: Gerencia hora e localiza√ß√£o (ip-api.com, com cache). persona_manager.py: Define a personalidade "Aurora" para o LLM. usage_tracker.py: Monitora e persiste o uso de APIs externas (ex: Google Search) em JSON para evitar limites.</p>
                <p>core/assistant.py: Prop√≥sito: Orquestrador principal. Fun√ß√µes: run(). Depend√™ncias: M√≥dulos internos. Racional: Controla o loop de intera√ß√£o, roteia inten√ß√µes (locais, controladas com limite, delegadas ao LLM).</p>
                <p>llm/generator.py: Prop√≥sito: Integra√ß√£o com Google Gemini e gerenciamento de ferramentas. Fun√ß√µes: generate_response(). Depend√™ncias: google.generativeai, tools, conscience. Racional: Usa Gemini 1.5-flash-latest para NLP avan√ßado e Function Calling, controlando o uso da busca web.</p>
                <p>memory/chat_history.py: Prop√≥sito: Manter hist√≥rico da conversa. Fun√ß√µes: add_message(), get_history(). Racional: Essencial para contexto do LLM, limita a 20 mensagens para otimiza√ß√£o.</p>
                <p>nlp/processing.py: Prop√≥sito: An√°lise inicial de inten√ß√µes. Fun√ß√µes: processar_texto(). Racional: Estrat√©gia h√≠brida (regras para b√°sico, LLM para complexo).</p>
                <p>tools/: Prop√≥sito: Encapsular APIs externas. weather_tool.py: Clima via WeatherAPI.com. web_search_tool.py: Pesquisa web via Google Custom Search API. Racional: Estende capacidades do assistente, encapsuladas para Function Calling.</p>
                <p>main.py: Prop√≥sito: Ponto de entrada do aplicativo. Depend√™ncias: core.assistant. Racional: Inicializa o assistente.</p>
                <p>Configura√ß√£o Essencial (.env): Arquivo para chaves de API (GEMINI_API_KEY, WEATHERAPI_API_KEY, Google Search_API_KEY, Google Search_CX). N√£o deve ser versionado. Instru√ß√µes detalhadas para obten√ß√£o de cada chave.</p>
                <p>Fluxo de Execu√ß√£o Detalhado: Exemplo de "Aurora, pesquise para mim o que √© computa√ß√£o qu√¢ntica." Mostra o passo a passo desde a entrada de voz, processamento de inten√ß√£o, verifica√ß√£o de limite de API, chamada ao LLM, execu√ß√£o da ferramenta de busca, e formula√ß√£o da resposta final pela IA.</p>
                <p>Padr√µes de Design e Metodologias: Arquitetura Modular/Camadas (alta coes√£o, baixo acoplamento), Inje√ß√£o de Depend√™ncia (via Construtor), Estrat√©gia H√≠brida de Processamento de Inten√ß√µes (Regras + LLM), Function Calling (Google Gemini), Gerenciamento de Estado de Conversa (Chat History), Gerenciamento de Recursos Persistente (Usage Tracker), Configura√ß√£o Externa (dotenv).</p>
                <p>Considera√ß√µes Finais: A arquitetura do Aurora √© robusta e escal√°vel. Futuras melhorias incluem mais ferramentas, hotword e UI sofisticada.</p>
            </div>
        </section>

    </main>

    <footer class="py-8 text-center border-t border-gray-200">
        <p class="text-sm text-gray-500">
            Documenta√ß√£o do Projeto Aurora &copy; 2025. Desenvolvido por <strong>Raphael Olimpio Aparecido Pereira Lima</strong> (Tec. Mecatr√¥nica, Estudante de Eng. de Software) com a assist√™ncia de <strong>Gemini IA</strong>.
        </p>
    </footer>

    <script>
        // Fix: Access Chart components directly from the global Chart object
        const { LineController, CategoryScale, LinearScale, PointElement, LineElement, Filler, ArcElement, BarController, BarElement, RadialLinearScale, BubbleController } = Chart;
        Chart.register(
            LineController,
            CategoryScale,
            LinearScale,
            PointElement,
            LineElement,
            Filler,
            ArcElement,
            BarController,
            BarElement,
            RadialLinearScale,
            BubbleController
        );

        const accordionButtons = document.querySelectorAll('.accordion-button');
        accordionButtons.forEach(button => {
            button.addEventListener('click', () => {
                const content = button.nextElementSibling;
                const arrow = button.querySelector('.arrow-icon');
                
                button.setAttribute('aria-expanded', content.style.maxHeight ? 'false' : 'true');

                if (content.style.maxHeight) {
                    content.style.maxHeight = null;
                    content.classList.remove('open');
                    arrow.classList.remove('open');
                } else {
                    content.style.maxHeight = content.scrollHeight + "px";
                    content.classList.add('open');
                    arrow.classList.add('open');
                }
            });
        });
        
        function processLabels(labels, maxLength = 16) {
            return labels.map(label => {
                if (typeof label === 'string' && label.length > maxLength) {
                    const words = label.split(' ');
                    const newLabel = [];
                    let currentLine = '';
                    words.forEach(word => {
                        if ((currentLine + word).length > maxLength) {
                            newLabel.push(currentLine.trim());
                            currentLine = word + ' ';
                        } else {
                            currentLine += word + ' ';
                        }
                    });
                    if (currentLine.trim()) newLabel.push(currentLine.trim());
                    return newLabel.length > 0 ? newLabel : [label];
                }
                return label;
            });
        }

        const tooltipTitleCallback = (tooltipItems) => {
            const item = tooltipItems[0];
            let label = item.chart.data.labels[item.dataIndex];
            if (Array.isArray(label)) {
                return label.join(' ');
            }
            return label;
        };
        
        const moduleCtx = document.getElementById('moduleComponentsChart').getContext('2d');
        const chartLabels = ['audio/', 'conscience/', 'core/', 'llm/', 'memory/', 'nlp/', 'tools/', 'main.py', '.env'];
        const processedChartLabels = processLabels(chartLabels);

        new Chart(moduleCtx, {
            type: 'bar',
            data: {
                labels: processedChartLabels,
                datasets: [{
                    label: 'N¬∫ de Arquivos Python Principais',
                    data: [1, 3, 1, 1, 1, 1, 2, 1, 1],
                    backgroundColor: '#A0AEC0',
                    borderColor: '#6C757D',
                    borderWidth: 1
                }]
            },
            options: {
                responsive: true,
                maintainAspectRatio: false,
                indexAxis: 'y',
                scales: {
                    x: {
                        beginAtZero: true,
                        ticks: {
                            color: '#495057',
                            stepSize: 1
                        },
                        grid: {
                            color: 'rgba(0,0,0,0.05)'
                        }
                    },
                    y: {
                        ticks: {
                            color: '#495057',
                            font: {
                                size: 11
                            }
                        },
                        grid: {
                            display: false
                        }
                    }
                },
                plugins: {
                    legend: {
                        display: false
                    },
                    tooltip: {
                        callbacks: {
                            title: function(tooltipItems) {
                                const item = tooltipItems[0];
                                if (Array.isArray(item.label)) {
                                  return item.label.join(' ');
                                }
                                return item.label;
                            }
                        }
                    }
                }
            }
        });

        document.querySelectorAll('nav a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                const targetId = this.getAttribute('href');
                const targetElement = document.querySelector(targetId);
                if (targetElement) {
                    const navHeight = document.querySelector('nav').offsetHeight;
                    const elementPosition = targetElement.getBoundingClientRect().top;
                    const offsetPosition = elementPosition + window.pageYOffset - navHeight - 20;

                    window.scrollTo({
                        top: offsetPosition,
                        behavior: "smooth"
                    });
                }
            });
        });

        // Gemini API integration for Summarize Section
        const summarizeButtons = document.querySelectorAll('.summarize-button');
        summarizeButtons.forEach(button => {
            button.addEventListener('click', async () => {
                const accordionContent = button.closest('.accordion-content');
                const summaryOutput = accordionContent.querySelector('.summary-output');
                const loadingIndicator = accordionContent.querySelector('.loading-indicator');
                
                // Clear previous output and show loading
                summaryOutput.innerHTML = '';
                loadingIndicator.classList.remove('hidden');

                let contentToSummarize = '';
                accordionContent.childNodes.forEach(node => {
                    if (node.nodeType === Node.ELEMENT_NODE && !node.classList.contains('llm-button-group') && !node.classList.contains('summary-output') && !node.classList.contains('loading-indicator')) {
                        contentToSummarize += node.innerText + '\n';
                    } else if (node.nodeType === Node.TEXT_NODE) {
                        contentToSummarize += node.textContent + '\n';
                    }
                });

                const prompt = `Resuma o seguinte texto sobre um m√≥dulo ou arquivo do projeto Aurora em portugu√™s, de forma concisa e clara: ${contentToSummarize.trim()}`;

                let chatHistory = [];
                chatHistory.push({ role: "user", parts: [{ text: prompt }] });
                const payload = { contents: chatHistory };
                const apiKey = "";
                const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;

                try {
                    const response = await fetch(apiUrl, {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json' },
                        body: JSON.stringify(payload)
                    });
                    const result = await response.json();

                    if (result.candidates && result.candidates.length > 0 &&
                        result.candidates[0].content && result.candidates[0].content.parts &&
                        result.candidates[0].content.parts.length > 0) {
                        const text = result.candidates[0].content.parts[0].text;
                        summaryOutput.innerHTML = `<strong>Resumo da Aurora:</strong> ${text}`;
                    } else {
                        summaryOutput.innerHTML = 'N√£o foi poss√≠vel gerar o resumo. Tente novamente.';
                    }
                } catch (error) {
                    console.error('Erro ao chamar a API Gemini para resumo:', error);
                    summaryOutput.innerHTML = 'Erro ao conectar com a IA para gerar o resumo.';
                } finally {
                    loadingIndicator.classList.add('hidden');
                }
            });
        });

        // Gemini API integration for Q&A about the project
        const userQuestionInput = document.getElementById('user-question');
        const askAuroraButton = document.getElementById('ask-aurora-button');
        const chatDisplay = document.getElementById('chat-display');
        const chatLoadingIndicator = document.getElementById('chat-loading-indicator');
        const fullDocText = document.getElementById('full-doc-text').innerText;

        let chatHistory = [{ role: "model", parts: [{ text: "Ol√°! Sou a Aurora, sua assistente de documenta√ß√£o. Como posso ajudar a explorar o projeto?" }] }];

        askAuroraButton.addEventListener('click', async () => {
            const userMessage = userQuestionInput.value.trim();
            if (!userMessage) return;

            userQuestionInput.value = '';
            displayChatMessage(userMessage, 'user');
            chatLoadingIndicator.classList.remove('hidden');
            chatDisplay.scrollTop = chatDisplay.scrollHeight; // Scroll to bottom

            const prompt = `Voc√™ √© um assistente de documenta√ß√£o chamado Aurora. Responda √† pergunta do usu√°rio APENAS com base no seguinte contexto do projeto Aurora. Se a informa√ß√£o n√£o estiver explicitamente no contexto fornecido, diga educadamente que n√£o pode responder a essa pergunta com base na documenta√ß√£o dispon√≠vel.
            
            Contexto do Projeto Aurora:
            ${fullDocText}

            Pergunta do Usu√°rio: ${userMessage}`;

            let currentChatHistory = [];
            currentChatHistory.push({ role: "user", parts: [{ text: prompt }] });

            const payload = { contents: currentChatHistory };
            const apiKey = "";
            const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;

            try {
                const response = await fetch(apiUrl, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify(payload)
                });
                const result = await response.json();

                if (result.candidates && result.candidates.length > 0 &&
                    result.candidates[0].content && result.candidates[0].content.parts &&
                    result.candidates[0].content.parts.length > 0) {
                    const auroraResponse = result.candidates[0].content.parts[0].text;
                    displayChatMessage(auroraResponse, 'aurora');
                } else {
                    displayChatMessage('Desculpe, n√£o consegui processar sua pergunta no momento. Por favor, tente novamente.', 'aurora');
                }
            } catch (error) {
                console.error('Erro ao chamar a API Gemini para Q&A:', error);
                displayChatMessage('Ocorreu um erro ao conectar com a IA. Por favor, verifique sua conex√£o ou tente mais tarde.', 'aurora');
            } finally {
                chatLoadingIndicator.classList.add('hidden');
                chatDisplay.scrollTop = chatDisplay.scrollHeight; // Scroll to bottom after response
            }
        });

        function displayChatMessage(message, sender) {
            const messageDiv = document.createElement('div');
            messageDiv.classList.add('chat-message', sender);
            messageDiv.innerText = message;
            chatDisplay.appendChild(messageDiv);
            chatDisplay.scrollTop = chatDisplay.scrollHeight; // Scroll to bottom
        }

    </script>
</body>
</html>

